#!/usr/bin/env python3
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†
"""

import os
import torch
import json
import argparse
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TransformersInference:
    """Transformersæ¨¡å‹æ¨ç†"""
    
    def __init__(self, model_path):
        from transformers import AutoTokenizer, AutoModelForTokenClassification
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        # æ ‡ç­¾æ˜ å°„
        self.id2label = {
            0: 'O', 1: 'B-TECH', 2: 'I-TECH',
            3: 'B-NUM', 4: 'I-NUM',
            5: 'B-UNIT', 6: 'I-UNIT'
        }
        
        logger.info(f"Transformersæ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
    
    def predict_text(self, text: str) -> Dict[str, Any]:
        """å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„æµ‹"""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=256
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=-1)
        
        # è·å–tokenså’Œæ ‡ç­¾
        tokens = self.tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        labels = [self.id2label[p.item()] for p in predictions[0]]
        
        # è¿‡æ»¤ç‰¹æ®Štokens
        result_tokens = []
        result_labels = []
        for token, label in zip(tokens, labels):
            if token not in ['[CLS]', '[SEP]', '[PAD]']:
                result_tokens.append(token)
                result_labels.append(label)
        
        # æå–å®ä½“
        entities = self._extract_entities(result_tokens, result_labels)
        
        return {
            'text': text,
            'tokens': result_tokens,
            'labels': result_labels,
            'entities': entities
        }
    
    def _extract_entities(self, tokens: List[str], labels: List[str]) -> List[Dict[str, str]]:
        """ä»é¢„æµ‹ç»“æœä¸­æå–å®ä½“"""
        entities = []
        current_entity = []
        current_type = None
        
        for token, label in zip(tokens, labels):
            if label.startswith('B-'):
                # å¼€å§‹æ–°å®ä½“
                if current_entity:
                    entities.append({
                        'text': ''.join(current_entity),
                        'type': current_type
                    })
                current_entity = [token]
                current_type = label[2:]
            elif label.startswith('I-') and current_type:
                # ç»§ç»­å½“å‰å®ä½“
                if label[2:] == current_type:
                    current_entity.append(token)
                else:
                    # ç±»å‹ä¸åŒ¹é…ï¼Œç»“æŸå½“å‰å®ä½“
                    if current_entity:
                        entities.append({
                            'text': ''.join(current_entity),
                            'type': current_type
                        })
                    current_entity = []
                    current_type = None
            else:
                # Oæ ‡ç­¾ï¼Œç»“æŸå½“å‰å®ä½“
                if current_entity:
                    entities.append({
                        'text': ''.join(current_entity),
                        'type': current_type
                    })
                current_entity = []
                current_type = None
        
        # å¤„ç†æœ€åä¸€ä¸ªå®ä½“
        if current_entity:
            entities.append({
                'text': ''.join(current_entity),
                'type': current_type
            })
        
        return entities

class SpacyInference:
    """spaCyæ¨¡å‹æ¨ç†"""
    
    def __init__(self, model_path):
        import spacy
        
        self.nlp = spacy.load(model_path)
        logger.info(f"spaCyæ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
    
    def predict_text(self, text: str) -> Dict[str, Any]:
        """å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œé¢„æµ‹"""
        doc = self.nlp(text)
        
        # è·å–tokenså’Œæ ‡ç­¾
        tokens = [token.text for token in doc]
        labels = []
        for token in doc:
            if token.ent_type_:
                labels.append(f"{token.ent_iob_}-{token.ent_type_}")
            else:
                labels.append('O')
        
        # æå–å®ä½“
        entities = []
        for ent in doc.ents:
            entities.append({
                'text': ent.text,
                'type': ent.label_
            })
        
        return {
            'text': text,
            'tokens': tokens,
            'labels': labels,
            'entities': entities
        }

def load_test_data(file_path: str) -> List[str]:
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # è¿‡æ»¤ç©ºè¡Œå’Œå»é™¤æ¢è¡Œç¬¦
    texts = [line.strip() for line in lines if line.strip()]
    return texts

def run_inference(framework: str, model_path: str, test_files: List[str]):
    """è¿è¡Œæ¨ç†"""
    print("="*80)
    print(f"{framework.upper()} æ¨¡å‹æ¨ç†")
    print("="*80)
    
    # åˆå§‹åŒ–æ¨¡å‹
    if framework == "transformers":
        model = TransformersInference(model_path)
    elif framework == "spacy":
        model = SpacyInference(model_path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¡†æ¶: {framework}")
    
    all_results = {}
    
    for test_file in test_files:
        if not os.path.exists(test_file):
            print(f"! æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
            continue
            
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {test_file}")
        print("-" * 60)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_texts = load_test_data(test_file)
        print(f"åŠ è½½äº† {len(test_texts)} è¡Œæ–‡æœ¬")
        
        results = []
        
        # å¤„ç†æ¯è¡Œæ–‡æœ¬
        for i, text in enumerate(test_texts[:10], 1):  # åªå¤„ç†å‰10è¡Œä½œä¸ºç¤ºä¾‹
            if len(text) > 200:  # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
                text = text[:200] + "..."
            
            print(f"\n[{i}] è¾“å…¥: {text}")
            
            try:
                result = model.predict_text(text)
                results.append(result)
                
                # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
                if result['entities']:
                    print("ğŸ¯ è¯†åˆ«çš„å®ä½“:")
                    for entity in result['entities']:
                        print(f"   â€¢ {entity['text']} ({entity['type']})")
                else:
                    print("   æ— å®ä½“è¯†åˆ«")
                    
                # æ˜¾ç¤ºè¯¦ç»†çš„tokenæ ‡ç­¾ï¼ˆå¯é€‰ï¼Œå¤ªå¤šæ—¶è·³è¿‡ï¼‰
                if len(result['tokens']) < 20:
                    print("ğŸ·ï¸  è¯¦ç»†æ ‡æ³¨:")
                    for token, label in zip(result['tokens'], result['labels']):
                        if label != 'O':
                            print(f"   {token} -> {label}")
                
            except Exception as e:
                print(f"   âŒ é¢„æµ‹å¤±è´¥: {e}")
                continue
        
        all_results[test_file] = results
        
        # ç»Ÿè®¡ç»“æœ
        total_entities = sum(len(r['entities']) for r in results)
        entity_types = {}
        for result in results:
            for entity in result['entities']:
                entity_type = entity['type']
                entity_types[entity_type] = entity_types.get(entity_type, 0) + 1
        
        print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"   æ€»å®ä½“æ•°: {total_entities}")
        if entity_types:
            print("   å®ä½“ç±»å‹åˆ†å¸ƒ:")
            for entity_type, count in entity_types.items():
                print(f"     {entity_type}: {count}")
    
    # ä¿å­˜ç»“æœ
    output_file = f"inference_results_{framework}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æ¨ç†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return all_results

def main():
    parser = argparse.ArgumentParser(description="æ¨¡å‹æ¨ç†è„šæœ¬")
    parser.add_argument(
        "--framework",
        type=str,
        choices=["transformers", "spacy"],
        default="transformers",
        help="é€‰æ‹©æ¨¡å‹æ¡†æ¶"
    )
    parser.add_argument(
        "--model-path",
        type=str,
        help="æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰"
    )
    parser.add_argument(
        "--test-files",
        nargs="+",
        default=["../data/zh.txt", "../data/en.txt"],
        help="æµ‹è¯•æ–‡ä»¶åˆ—è¡¨"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤æ¨¡å‹è·¯å¾„
    if not args.model_path:
        if args.framework == "transformers":
            args.model_path = "./trained_models/transformers_offline"
        elif args.framework == "spacy":
            args.model_path = "./trained_models/spacy_offline"
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
        print("\nè¯·å…ˆè®­ç»ƒæ¨¡å‹:")
        print(f"python train_offline_simple.py --framework {args.framework} --epochs 5")
        return
    
    print(f"ğŸš€ å¼€å§‹æ¨ç†...")
    print(f"   æ¡†æ¶: {args.framework}")
    print(f"   æ¨¡å‹: {args.model_path}")
    print(f"   æµ‹è¯•æ–‡ä»¶: {args.test_files}")
    
    # è¿è¡Œæ¨ç†
    results = run_inference(args.framework, args.model_path, args.test_files)
    
    print("\nâœ… æ¨ç†å®Œæˆ!")

if __name__ == "__main__":
    main()