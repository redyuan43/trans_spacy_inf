#!/usr/bin/env python3
"""
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æµ‹è¯•dataç›®å½•ä¸‹çš„zh.txtå’Œen.txtæ–‡ä»¶
"""

import json
import sys
import argparse
from sentence_eos_processor import SentenceEOSProcessor

def test_transformers_model(file_path, enable_eos=True):
    """ä½¿ç”¨Transformersæ¨¡å‹æµ‹è¯•"""
    import torch
    from transformers import AutoTokenizer, AutoModelForTokenClassification

    print(f"=== ä½¿ç”¨Transformersæ¨¡å‹æµ‹è¯• {file_path} ===")

    # åŠ è½½æ¨¡å‹
    model_path = "./trained_models/transformers_offline"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForTokenClassification.from_pretrained(model_path)
    model.eval()

    # æ ‡ç­¾æ˜ å°„
    id2label = {0: 'O', 1: 'B-TECH', 2: 'I-TECH', 3: 'B-NUM', 4: 'I-NUM', 5: 'B-UNIT', 6: 'I-UNIT'}

    # EOSå¤„ç†å™¨
    eos_processor = None
    if enable_eos:
        eos_processor = SentenceEOSProcessor()
        print("å·²å¯ç”¨å¥å­ç»“æŸç¬¦å·(EOS)è¯†åˆ«")
    
    # è¯»å–æµ‹è¯•æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    results = []
    
    for i, line in enumerate(lines[:10], 1):  # åªæµ‹è¯•å‰10è¡Œ
        text = line.strip()
        if not text:
            continue
            
        print(f"\n--- ç¬¬{i}è¡Œ ---")
        print(f"åŸæ–‡: {text}")
        
        # åˆ†è¯å’Œé¢„æµ‹
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=2)
        
        # è½¬æ¢ä¸ºtokenså’Œlabels
        tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
        labels = [id2label[p.item()] for p in predictions[0]]
        
        # æå–å®ä½“
        entities = []
        current_entity = []
        current_label = None
        current_text = []
        
        for token, label in zip(tokens, labels):
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                continue
                
            # å¤„ç†WordPiece tokens
            if token.startswith('##'):
                token_text = token[2:]
            else:
                token_text = token
                
            if label.startswith('B-'):
                # ä¿å­˜å‰ä¸€ä¸ªå®ä½“
                if current_entity:
                    entities.append((''.join(current_text), current_label))
                # å¼€å§‹æ–°å®ä½“
                current_entity = [token]
                current_text = [token_text]
                current_label = label[2:]
            elif label.startswith('I-') and current_label == label[2:]:
                current_entity.append(token)
                current_text.append(token_text)
            else:
                # ç»“æŸå½“å‰å®ä½“
                if current_entity:
                    entities.append((''.join(current_text), current_label))
                    current_entity = []
                    current_text = []
                    current_label = None
        
        # å¤„ç†æœ€åä¸€ä¸ªå®ä½“
        if current_entity:
            entities.append((''.join(current_text), current_label))
        
        print(f"è¯†åˆ«çš„å®ä½“: {entities}")
        
        # è½¬æ¢å®ä½“æ ¼å¼ä¸º(start, end, label)
        entities_tuple_format = []
        char_pos = 0
        for token in tokens:
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                continue
            token_text = token[2:] if token.startswith('##') else token

            # æŸ¥æ‰¾å®ä½“
            for entity_text, entity_label in entities:
                if text[char_pos:char_pos+len(entity_text)] == entity_text:
                    entities_tuple_format.append((char_pos, char_pos+len(entity_text), entity_label))

            char_pos += len(token_text)

        result = {
            'text': text,
            'tokens': [t for t in tokens if t not in ['[CLS]', '[SEP]', '[PAD]']],
            'labels': [l for t, l in zip(tokens, labels) if t not in ['[CLS]', '[SEP]', '[PAD]']],
            'entities': entities_tuple_format
        }

        # æ·»åŠ EOSåå¤„ç†
        if eos_processor:
            result = eos_processor.process_prediction_result(text, result)

        results.append(result)
    
    return results

def test_spacy_model(file_path, enable_eos=True):
    """ä½¿ç”¨spaCyæ¨¡å‹æµ‹è¯•"""
    import spacy

    print(f"\n=== ä½¿ç”¨spaCyæ¨¡å‹æµ‹è¯• {file_path} ===")

    try:
        # åŠ è½½æ¨¡å‹
        nlp = spacy.load("./trained_models/spacy_offline")
    except Exception as e:
        print(f"åŠ è½½spaCyæ¨¡å‹å¤±è´¥: {e}")
        return []

    # EOSå¤„ç†å™¨
    eos_processor = None
    if enable_eos:
        eos_processor = SentenceEOSProcessor()
        print("å·²å¯ç”¨å¥å­ç»“æŸç¬¦å·(EOS)è¯†åˆ«")
    
    # è¯»å–æµ‹è¯•æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    results = []
    
    for i, line in enumerate(lines[:10], 1):  # åªæµ‹è¯•å‰10è¡Œ
        text = line.strip()
        if not text:
            continue
            
        print(f"\n--- ç¬¬{i}è¡Œ ---")
        print(f"åŸæ–‡: {text}")
        
        # é¢„æµ‹
        doc = nlp(text)
        
        # æå–ç»“æœ
        tokens = [token.text for token in doc]
        labels = [f"{token.ent_iob_}-{token.ent_type_}" if token.ent_iob_ != 'O' else 'O' for token in doc]
        entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]

        print(f"åˆ†è¯ç»“æœ: {tokens}")
        print(f"è¯†åˆ«çš„å®ä½“: {[(text[start:end], label) for start, end, label in entities]}")

        result = {
            'text': text,
            'tokens': tokens,
            'labels': labels,
            'entities': entities
        }

        # æ·»åŠ EOSåå¤„ç†
        if eos_processor:
            result = eos_processor.process_prediction_result(text, result)

        results.append(result)
    
    return results

def test_spacy_transformers_model(file_path, enable_eos=True):
    """ä½¿ç”¨SpaCy-Transformersæ¨¡å‹æµ‹è¯•"""
    import spacy

    print(f"\n=== ä½¿ç”¨SpaCy-Transformersæ¨¡å‹æµ‹è¯• {file_path} ===")

    try:
        # åŠ è½½æ¨¡å‹
        nlp = spacy.load("./trained_models/spacy_transformers")
    except Exception as e:
        print(f"åŠ è½½SpaCy-Transformersæ¨¡å‹å¤±è´¥: {e}")
        return []

    # EOSå¤„ç†å™¨
    eos_processor = None
    if enable_eos:
        eos_processor = SentenceEOSProcessor()
        print("å·²å¯ç”¨å¥å­ç»“æŸç¬¦å·(EOS)è¯†åˆ«")

    # è¯»å–æµ‹è¯•æ–‡ä»¶
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    results = []

    for i, line in enumerate(lines[:10], 1):  # åªæµ‹è¯•å‰10è¡Œ
        text = line.strip()
        if not text:
            continue

        print(f"\n--- ç¬¬{i}è¡Œ ---")
        print(f"åŸæ–‡: {text}")

        try:
            # é¢„æµ‹
            doc = nlp(text)

            # æå–ç»“æœ
            tokens = [token.text for token in doc]
            labels = [f"{token.ent_iob_}-{token.ent_type_}" if token.ent_iob_ != 'O' else 'O' for token in doc]
            entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]

            print(f"åˆ†è¯ç»“æœ: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
            print(f"è¯†åˆ«çš„å®ä½“: {[(text[start:end], label) for start, end, label in entities]}")

            result = {
                'text': text,
                'tokens': tokens,
                'labels': labels,
                'entities': entities
            }

            # æ·»åŠ EOSåå¤„ç†
            if eos_processor:
                result = eos_processor.process_prediction_result(text, result)

            results.append(result)

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥: {e}")
            continue

    return results

def main():
    parser = argparse.ArgumentParser(description='æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹')
    parser.add_argument('--framework', choices=['transformers', 'spacy', 'spacy_transformers', 'all'],
                       default='all', help='é€‰æ‹©æµ‹è¯•çš„æ¡†æ¶')
    parser.add_argument('--file', choices=['zh', 'en', 'both'],
                       default='both', help='é€‰æ‹©æµ‹è¯•çš„æ–‡ä»¶')
    
    args = parser.parse_args()
    
    files_to_test = []
    if args.file in ['zh', 'both']:
        files_to_test.append('data/zh.txt')
    if args.file in ['en', 'both']:
        files_to_test.append('data/en.txt')
    
    all_results = {}
    
    for file_path in files_to_test:
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•æ–‡ä»¶: {file_path}")
        print(f"{'='*60}")
        
        file_results = {}

        if args.framework in ['transformers', 'all']:
            try:
                transformer_results = test_transformers_model(file_path, enable_eos=True)
                file_results['transformers'] = transformer_results
            except Exception as e:
                print(f"Transformersæµ‹è¯•å¤±è´¥: {e}")

        if args.framework in ['spacy', 'all']:
            try:
                spacy_results = test_spacy_model(file_path, enable_eos=True)
                file_results['spacy'] = spacy_results
            except Exception as e:
                print(f"spaCyæµ‹è¯•å¤±è´¥: {e}")

        if args.framework in ['spacy_transformers', 'all']:
            try:
                spacy_transformers_results = test_spacy_transformers_model(file_path, enable_eos=True)
                file_results['spacy_transformers'] = spacy_transformers_results
            except Exception as e:
                print(f"SpaCy-Transformersæµ‹è¯•å¤±è´¥: {e}")
        
        all_results[file_path] = file_results
    
    # ä¿å­˜ç»“æœ
    output_file = f"validation_results_{args.framework}_{args.file}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)

    print(f"\nâœ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
    print(f"\n{'='*60}")
    print("ğŸ“Š æµ‹è¯•æ€»ç»“")
    print(f"{'='*60}")
    for file_path, file_data in all_results.items():
        print(f"\nğŸ“ æ–‡ä»¶: {file_path}")
        for framework, results in file_data.items():
            if results:
                print(f"  âœ… {framework}: {len(results)} æ¡ç»“æœ")
            else:
                print(f"  âŒ {framework}: æµ‹è¯•å¤±è´¥æˆ–æ— ç»“æœ")

if __name__ == "__main__":
    main()