# NER模型评估指南

## 一、评估指标详解

### 1.1 核心指标

#### Precision（精确率）
```
精确率 = 正确识别的实体数 / 模型识别出的总实体数
```
- **含义**：模型识别的实体中，有多少是正确的
- **高精确率**：很少误报（把非实体识别为实体）
- **示例**：模型识别10个实体，8个正确 → 精确率80%

#### Recall（召回率）
```
召回率 = 正确识别的实体数 / 实际存在的总实体数
```
- **含义**：所有实体中，有多少被模型找到了
- **高召回率**：很少漏报（遗漏实体）
- **示例**：文本有10个实体，模型找到7个 → 召回率70%

#### F1 Score（F1分数）
```
F1 = 2 × (精确率 × 召回率) / (精确率 + 召回率)
```
- **含义**：精确率和召回率的调和平均
- **最佳平衡**：F1分数高表示精确率和召回率都好
- **范围**：0-1，越接近1越好

### 1.2 训练过程指标

#### Loss（损失）
- **含义**：模型预测与真实标签的差异
- **趋势**：应该逐渐下降
- **警告信号**：
  - 损失不下降 → 学习率可能太小
  - 损失震荡 → 学习率可能太大
  - 损失先降后升 → 过拟合

#### Learning Rate（学习率）
- **作用**：控制模型参数更新的步长
- **典型值**：1e-5 到 5e-5（Transformers）
- **调整策略**：通常使用warmup和线性衰减

## 二、如何查看训练结果

### 2.1 Transformers训练输出

```python
{'loss': 1.4398, 'grad_norm': 4.56, 'learning_rate': 2.76e-05, 'epoch': 1.0}
{'loss': 0.7074, 'grad_norm': 2.30, 'learning_rate': 2.46e-05, 'epoch': 2.0}
{'loss': 0.3705, 'grad_norm': 1.67, 'learning_rate': 2.16e-05, 'epoch': 3.0}
```

**解读**：
- `loss`: 从1.44降到0.37 → ✅ 正常学习
- `grad_norm`: 梯度范数，稳定在1-5之间较好
- `learning_rate`: 逐渐降低（线性衰减）
- `epoch`: 当前训练轮数

### 2.2 SpaCy训练输出

```python
Epoch 2/10, 损失: 732.85
Epoch 4/10, 损失: 438.39
Epoch 6/10, 损失: 17.25
Epoch 8/10, 损失: 11.61
```

**解读**：
- 损失从732降到11 → ✅ 快速收敛
- 但降得太快可能过拟合 → ⚠️ 需要验证集检查

### 2.3 评估结果

```python
F-score: 0.856
Precision: 0.892
Recall: 0.823
```

**解读**：
- F1=0.856 → 良好的综合性能
- P=0.892 > R=0.823 → 模型偏保守，宁缺毋滥

## 三、判断模型好坏

### 3.1 良好模型的特征

| 指标 | 优秀 | 良好 | 及格 | 需改进 |
|------|------|------|------|--------|
| F1分数 | >0.90 | 0.80-0.90 | 0.70-0.80 | <0.70 |
| 精确率 | >0.92 | 0.85-0.92 | 0.75-0.85 | <0.75 |
| 召回率 | >0.88 | 0.80-0.88 | 0.70-0.80 | <0.70 |
| 训练损失 | <0.1 | 0.1-0.3 | 0.3-0.5 | >0.5 |

### 3.2 问题诊断

#### 症状1：高精确率，低召回率
```
Precision: 0.95
Recall: 0.60
```
- **问题**：模型太保守，只识别非常确定的实体
- **解决**：
  - 增加训练数据
  - 降低分类阈值
  - 增加正样本权重

#### 症状2：低精确率，高召回率
```
Precision: 0.65
Recall: 0.92
```
- **问题**：模型太激进，把很多非实体也标注了
- **解决**：
  - 增加负样本
  - 提高分类阈值
  - 加强正则化

#### 症状3：训练好但测试差
```
训练F1: 0.95
测试F1: 0.65
```
- **问题**：严重过拟合
- **解决**：
  - 减少训练轮数
  - 增加dropout
  - 增加数据量
  - 使用数据增强

#### 症状4：损失不下降
```
Epoch 1: Loss 2.3
Epoch 10: Loss 2.2
```
- **问题**：模型没有学习
- **解决**：
  - 增大学习率
  - 检查数据格式
  - 检查标签是否正确

## 四、实体级别评估

### 4.1 查看各类实体表现

```python
实体类型 | Precision | Recall | F1-Score | Support
---------|-----------|--------|----------|--------
TECH     | 0.88      | 0.85   | 0.86     | 234
NUM      | 0.92      | 0.90   | 0.91     | 456
UNIT     | 0.86      | 0.82   | 0.84     | 312
---------|-----------|--------|----------|--------
Overall  | 0.89      | 0.86   | 0.87     | 1002
```

**分析**：
- NUM表现最好 → 数字模式简单
- UNIT稍差 → 可能单位种类太多
- TECH中等 → 技术术语需要更多样本

### 4.2 混淆矩阵分析

```
真实\预测  TECH  NUM  UNIT  O
TECH       180   2    8     44
NUM        1     410  3     42
UNIT       5     4    250   53
O          20    15   30    8937
```

**发现问题**：
- TECH有44个被误判为O → 漏识别问题
- O有65个被误判为实体 → 误识别问题

## 五、实时监控训练

### 5.1 使用TensorBoard（Transformers）

```bash
# 训练时添加
--logging_dir ./logs

# 查看
tensorboard --logdir ./logs
```

### 5.2 使用Weights & Biases

```python
# 安装
pip install wandb

# 训练时添加
--report_to wandb
```

### 5.3 自定义监控

```python
# 保存训练历史
history = {
    'loss': [],
    'f1': [],
    'precision': [],
    'recall': []
}

# 每个epoch后记录
history['loss'].append(loss)
history['f1'].append(f1_score)

# 绘制曲线
import matplotlib.pyplot as plt
plt.plot(history['loss'])
plt.title('Training Loss')
plt.show()
```

## 六、最佳实践

### 6.1 训练策略

1. **先快速实验**
   - 用小数据集（100条）
   - 训练3-5轮
   - 快速验证方案可行性

2. **逐步优化**
   - 增加数据到500-1000条
   - 调整超参数
   - 增加训练轮数

3. **防止过拟合**
   - 使用验证集（20%数据）
   - Early Stopping
   - 正则化（dropout、weight decay）

### 6.2 评估策略

1. **多维度评估**
   - 不只看F1，也看P和R
   - 分实体类型评估
   - 测试不同领域数据

2. **错误分析**
   - 收集预测错误的样本
   - 分析错误模式
   - 针对性改进

3. **人工审核**
   - 随机抽取100条预测结果
   - 人工检查质量
   - 计算人工一致性

## 七、评估脚本示例

```python
def evaluate_model(model, test_data):
    """完整的模型评估"""

    predictions = []
    labels = []

    # 收集预测结果
    for text, true_entities in test_data:
        pred_entities = model.predict(text)
        predictions.append(pred_entities)
        labels.append(true_entities)

    # 计算指标
    from sklearn.metrics import classification_report
    report = classification_report(
        labels,
        predictions,
        output_dict=True
    )

    # 输出结果
    print(f"整体性能:")
    print(f"  F1-Score: {report['weighted avg']['f1-score']:.3f}")
    print(f"  Precision: {report['weighted avg']['precision']:.3f}")
    print(f"  Recall: {report['weighted avg']['recall']:.3f}")

    print(f"\n各类实体性能:")
    for label in ['TECH', 'NUM', 'UNIT']:
        if label in report:
            print(f"  {label}:")
            print(f"    F1: {report[label]['f1-score']:.3f}")
            print(f"    P: {report[label]['precision']:.3f}")
            print(f"    R: {report[label]['recall']:.3f}")

    return report
```

## 八、常见评估误区

### ❌ 误区1：只看损失
损失降低不代表性能提升，可能只是过拟合

### ❌ 误区2：只看F1
高F1可能掩盖类别不平衡问题

### ❌ 误区3：只用训练集评估
必须用独立的测试集

### ❌ 误区4：忽视业务需求
- 信息抽取：重视召回率
- 自动标注：重视精确率

## 九、调优建议

### 9.1 提升精确率
- 增加负样本
- 提高置信度阈值
- 后处理规则过滤

### 9.2 提升召回率
- 增加训练数据
- 数据增强
- 降低置信度阈值
- 集成多个模型

### 9.3 提升F1
- 平衡数据集
- 优化超参数
- 使用更大的预训练模型
- 改进特征工程

## 十、评估检查清单

- [ ] 使用独立测试集了吗？
- [ ] 查看各类实体的表现了吗？
- [ ] 分析错误案例了吗？
- [ ] 在真实数据上测试了吗？
- [ ] 考虑业务需求了吗？
- [ ] 与基线模型对比了吗？
- [ ] 测试了边界情况吗？
- [ ] 评估了推理速度吗？

通过系统的评估，才能真正了解模型的优势和不足，指导后续优化方向。